    a.  Write an OpenMP program which performs C=A+B  & D=A-B in separate blocks/sections where A,B,C & D  are arrays.

#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#define N 50
int main (int argc, char *argv[])
{
int i, nthreads, tid;
float a[N], b[N], c[N], d[N];

for (i=0; i<N; i++)
{
a[i] = i * 2;
b[i] = i + 2;
c[i] = d[i] = 0.0;
}

#pragma omp parallel shared (a,b,c,d,nthreads) private(i,tid)
{
tid = omp_get_thread_num();
if (tid == 0)
{
nthreads = omp_get_num_threads();
printf("Number of threads = %d\n", nthreads);
}
printf("Thread %d starting...\n",tid);

#pragma omp sections nowait
{

    #pragma omp section
    {
    printf("Thread %d doing section 1\n",tid);
    for (i=0; i<N; i++)
    {
   	 c[i] = a[i] + b[i];
   	 printf("Thread %d: c[%d]= %f\n",tid,i,c[i]);
    }
    }

    #pragma omp section
    {
    printf("Thread %d doing section 2\n",tid);
    for (i=0; i<N; i++)
    {
   	 d[i] = a[i] - b[i];
   	 printf("Thread %d: d[%d]= %f\n",tid,i,d[i]);
    }
    }
}
printf("Thread %d done.\n",tid);
}
}

Output:
Thread 3 starting...
Thread 3 doing section 1
Thread 3: c[0]= 2.000000
Thread 3: c[1]= 5.000000
Thread 3: c[2]= 8.000000
…...
Thread 3: c[48]= 146.000000
Thread 3: c[49]= 149.000000
Thread 3 doing section 2
Thread 3: d[0]= -2.000000
Thread 3: d[1]= -1.000000
Thread 3: d[2]= 0.000000
Thread 3: d[3]= 1.000000
….
Thread 3: d[27]= 25.000000
Thread 2 starting...
Thread 2 done.
Thread 1 starting...
Thread 1 done.
Number of threads = 4
Thread 0 starting...
Thread 0 done.
Thread 3: d[28]= 26.000000
...
Thread 3: d[48]= 46.000000
Thread 3: d[49]= 47.000000
Thread 3 done.
    b.   Write a MPI program to send the message from a process whose rank=3 to all other remaining processes.

#include <stdio.h>
#include <mpi.h>
#include <string.h>
#define BUFFER_SIZE 20

int main(int argc,char *argv[])
{
	int rank,size, Destination, i;
	int  root = 3, temp = 1;
	char msg[BUFFER_SIZE];
    
	MPI_Init(&argc,&argv);
	MPI_Status status;
	MPI_Comm_rank(MPI_COMM_WORLD,&rank);
	MPI_Comm_size(MPI_COMM_WORLD,&size);

    if(rank == 3)   	 
   {
  //  system("hostname");
    strcpy(msg, "Hello ");
    for (temp=0; temp<size ;temp++)
    {
    MPI_Send(msg, BUFFER_SIZE, MPI_CHAR, temp, 0,MPI_COMM_WORLD);
    }
    }

else
{     	 
//system("hostname");
MPI_Recv(msg, BUFFER_SIZE, MPI_CHAR, root, 0,MPI_COMM_WORLD, &status);
printf("\n%s in process with rank %d from Process with rank %d\n", msg,rank,root);
}

 MPI_Finalize();

return 0;
}


Output:
Hello  in process with rank 0 from Process with rank 3
Hello  in process with rank 2 from Process with rank 3
Hello  in process with rank 1 from Process with rank 3

    a. Write an OpenMP program which demonstrates the usage of Critical Directive.
//finding the largest element - example 
#include <stdio.h>
#include <omp.h>
#include<stdlib.h>

int main()
{
int *array, i, N, max, c;
printf("Enter the number of elements\n");
scanf("%d", &N);

if (N <= 0)
{
printf("The array elements cannot be stored\n");
exit(1);
}

array = (int *) malloc(sizeof(int) * N);

for (i = 0; i < N; i++)
array[i] = rand()%1000;

if (N== 1)
{
printf("The Largest Number In The Array is %d", array[0]);
exit(1);
}

max = 0;
omp_set_num_threads(8);
#pragma omp parallel for
for (i = 0; i < N; i = i + 1)
{
if (array[i] > max)
#pragma omp critical
if (array[i] > max)
max = array[i];
}

/* Serial Calculation */
c = array[0];
for (i = 1; i < N ; i++)
if (array[i] > c )
c  = array[i];

printf("The Input Array Elements Are \n");
for (i = 0; i < N ; i++)
printf("\t%d", array[i]);
printf("\n");

/* Checking For Output Validity */
if (c  == max)
printf("The Max Value Is Same From Serial And Parallel OpenMP Directive");
else
{
printf("The Max Value Is Not Same In Serial And Parallel OpenMP Directive");
exit(1);
}
printf("\n");
free(array);
printf("The Largest Number In The Given Array Is %d\n", max);
}
Output:
Enter the number of elements
20
The Input Array Elements Are
    383    886    777    915    793    335    386    492    649    421    362    27    690    59    763    926    540    426    172    736
The Max Value Is Same From Serial And Parallel OpenMP Directive
The Largest Number In The Given Array Is 926
    b. Write a CUDA program to demonstrate different types of memory.
#include <stdio.h>

__global__ void LM(float in)
{
//	float f;    
//	f = in; 	 
}

__global__ void GM(float *array)
{
	array[threadIdx.x] = 2.0f * (float) threadIdx.x;
}

__global__ void SM(float *array)
{
	int i, index = threadIdx.x;
	float average, sum = 0.0f;

     	__shared__ float sh_arr[128];

	sh_arr[index] = array[index];

	__syncthreads();

for (i=0; i<index; i++) 
{ 
sum += sh_arr[i]; 
}
	average = sum / (index + 1.0f);

	printf("Thread id = %d\t Average = %f\n",index,average);

	if (array[index] > average) { array[index] = average; }
	sh_arr[index] = 3.14;
}

int main(int argc, char **argv)
{
   LM<<<1, 128>>>(2.0f);

float h_arr[128];
float *d_arr;

cudaMalloc((void **) &d_arr, sizeof(float) * 128);
cudaMemcpy((void *)d_arr, (void *)h_arr, sizeof(float) * 128, cudaMemcpyHostToDevice);
GM<<<1, 128>>>(d_arr);
cudaMemcpy((void *)h_arr, (void *)d_arr, sizeof(float) * 128, cudaMemcpyDeviceToHost);

SM<<<1, 128>>>(d_arr);
cudaMemcpy((void *)h_arr, (void *)d_arr, sizeof(float) * 128, cudaMemcpyHostToDevice);

    	cudaDeviceSynchronize();
	return 0;
}

Output: 
Thread id = 96     Average = 94.020622
Thread id = 97     Average = 95.020409
Thread id = 98     Average = 96.020203
Thread id = 99     Average = 97.019997
Thread id = 100     Average = 98.019798
Thread id = 101     Average = 99.019608
Thread id = 96     Average = 94.020622
Thread id = 97     Average = 95.020409
Thread id = 98     Average = 96.020203
Thread id = 99     Average = 97.019997
Thread id = 100     Average = 98.019798
Thread id = 101     Average = 99.019608
.
.
.
Thread id = 60     Average = 58.032787
Thread id = 61     Average = 59.032257
Thread id = 62     Average = 60.031746
Thread id = 63     Average = 61.031250

//refer lab manual for explanation 
    a. Write an OpenMP program to add all the elements of two arrays A & B each of size 1000 and store their sum in a variable using reduction clause.

#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
int main (int argc, char *argv[])
{
int i, n =1000, a[1000], b[1000], sum;

for (i=0; i < n; i++)
a[i] = b[i] = i;
sum = 0.0;

#pragma omp parallel for reduction(+:sum)
for (i=0; i < n; i++)
sum = sum + (a[i] + b[i]);
printf(" Sum = %d\n",sum);

}

Output:
Sum = 999000 

    b.  Write a MPI program to calculate and print the value of PI.
#include <mpi.h>
#include<stdio.h>
#include<math.h>

double func(double x)
{
	return (4.0 / (1.0 + x*x));
}

int main(int argc,char *argv[])
{
	int	n,i,rank,size;
	double mypi, pi, h, sum, x;
	double PI25DT = 3.141592653589793238462643;

	MPI_Init(&argc,&argv);
	MPI_Comm_size(MPI_COMM_WORLD,&size);
	MPI_Comm_rank(MPI_COMM_WORLD,&rank);

	if(rank == 0){
	printf("\nEnter the number of intervals : ");
	scanf("%d",&n);
	}
    	 
   MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);


   h   = 1.0 / (double)n;
   sum = 0.0;
   for(i = rank + 1; i <= n; i +=size){
   	x = h * ((double)i - 0.5);
   	sum += func(x);
   }
   mypi = h * sum;

   
   MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

   if(rank == 0){
  	printf("pi is approximately %.16f, Error is %.16f\n",
pi, fabs(pi - PI25DT));
   }

   MPI_Finalize();
}

    a. Write an OpenMP program  to multiply two matrices A & B and find the resultant matrix C
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>

#define NRA 5
#define NCA 5
#define NCB 5

int main (int argc, char *argv[])
{
int tid, nthreads, i, j, k, chunk;
double a[NRA][NCA],b[NCA][NCB],c[NRA][NCB];
chunk = 2;

#pragma omp parallel shared(a,b,c,nthreads,chunk) private(tid,i,j,k)
{
tid = omp_get_thread_num();
if (tid == 0)
{
nthreads = omp_get_num_threads();
printf("Starting matrix multiple example with %d threads\n",nthreads);
printf("Initializing matrices...\n");
}
#pragma omp for schedule (static, chunk)
for (i=0; i<NRA; i++)
for (j=0; j<NCA; j++)
a[i][j]= i+j;

#pragma omp for schedule (static, chunk)
for (i=0; i<NCA; i++)
for (j=0; j<NCB; j++)
b[i][j]= i*j;

#pragma omp for schedule (static, chunk)
for (i=0; i<NRA; i++)
for (j=0; j<NCB; j++)
c[i][j]= 0;

/*** Do matrix multiply sharing iterations on outer loop ***/
/*** Display who does which iterations for demonstration purposes ***/
printf("Thread %d starting matrix multiply...\n",tid);
    #pragma omp for schedule (static, chunk)
    for (i=0; i<NRA; i++)
    {
    printf("Thread=%d did row=%d\n",tid,i);
    for(j=0; j<NCB; j++)
    for (k=0; k<NCA; k++)
    c[i][j] += a[i][k] * b[k][j];
    }
}

//End of parallel region 

printf("A Matrix:\n");
for (i=0; i<NRA; i++)
{
    for (j=0; j<NCA; j++)
    printf("%1.1f ", a[i][j]);
    printf("\n");
}

printf("B Matrix:\n");
for (i=0; i<NCA; i++)
{
    for (j=0; j<NCB; j++)
    printf("%6.1f ", b[i][j]);
    printf("\n");
}
printf("*****************\n");
printf("Result Matrix:\n");
for (i=0; i<NRA; i++)
{
    for (j=0; j<NCB; j++)
    printf("%6.1f ", c[i][j]);
    printf("\n");
}
printf("***************\n");
}
Output:
Starting matrix multiple example with 4 threads
Initializing matrices...
Thread 3 starting matrix multiply...
Thread 0 starting matrix multiply...
Thread=0 did row=0
Thread=0 did row=1
Thread 1 starting matrix multiply...
Thread=1 did row=2
Thread=1 did row=3
Thread 2 starting matrix multiply...
Thread=2 did row=4
A Matrix:
   0.0	1.0	2.0	3.0	4.0
   1.0	2.0	3.0	4.0	5.0
   2.0	3.0	4.0	5.0	6.0
   3.0	4.0	5.0	6.0	7.0
   4.0	5.0	6.0	7.0	8.0
B Matrix:
   0.0	0.0	0.0	0.0	0.0
   0.0	1.0	2.0	3.0	4.0
   0.0	2.0	4.0	6.0	8.0
   0.0	3.0	6.0	9.0   12.0
   0.0	4.0	8.0   12.0   16.0
*****************
Result Matrix:
   0.0   30.0   60.0   90.0  120.0
   0.0   40.0   80.0  120.0  160.0
   0.0   50.0  100.0  150.0  200.0
   0.0   60.0  120.0  180.0  240.0
   0.0   70.0  140.0  210.0  280.0
***************
    b.  Write a MPI program to send the message from a process whose rank=3 to all other remaining processes.

#include <stdio.h>
#include <mpi.h>
#include <string.h>
#define BUFFER_SIZE 20

int main(int argc,char *argv[])
{
	int rank,size, Destination, i;
	int  root = 3, temp = 1;
	char msg[BUFFER_SIZE];
    
	MPI_Init(&argc,&argv);
	MPI_Status status;
	MPI_Comm_rank(MPI_COMM_WORLD,&rank);
	MPI_Comm_size(MPI_COMM_WORLD,&size);

   if(rank == 3)   	 
   {
   // system("hostname");
    strcpy(msg, "Hello ");
    for (temp=0; temp<size;temp++)
    {
    MPI_Send(msg, BUFFER_SIZE, MPI_CHAR, temp, 0,MPI_COMM_WORLD);
    }
     }

else
{     	 
//system("hostname");
MPI_Recv(msg, BUFFER_SIZE, MPI_CHAR, root, 0,MPI_COMM_WORLD, &status);
printf("\n%s in process with rank %d from Process with rank %d\n", msg,rank,root);
}

 MPI_Finalize();

return 0;

}

Output:
Hello  in process with rank 0 from Process with rank 3
Hello  in process with rank 2 from Process with rank 3
Hello  in process with rank 1 from Process with rank 3

